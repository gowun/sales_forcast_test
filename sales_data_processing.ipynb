{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2b1958-6172-4f3a-a6da-5c2250f38e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46228edd-6695-4650-b431-ccd36dc48461",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import json\n",
    "import pickle\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "abs_data = pd.concat([\n",
    "    pd.read_csv('train_bert_data/train_7days.csv'),\n",
    "    pd.read_csv('train_bert_data/val_7days.csv'),\n",
    "    pd.read_csv('train_bert_data/test_7days.csv')\n",
    "])\n",
    "abs_data.shape, np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e18224-30c8-41f2-be52-cefbb164887e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "days_len = 7\n",
    "items = sorted(train['item'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe45e5a-472f-4dda-91ee-0914e9f26ef5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train[(train['date'] >= '2017-01-01')]\n",
    "train_gp = train.sort_values('date').groupby(['item', 'store', 'date'], as_index=False)\n",
    "train_gp = train_gp.agg({'sales':['mean']})\n",
    "train_gp.columns = ['item', 'store', 'date', 'sales']\n",
    "train_gp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdbb51e-2aef-4192-85c9-fc1b30152aa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = train\n",
    "window = 29\n",
    "lag = \n",
    "cols, names = list(), list()\n",
    "# Input sequence (t-n, ... t-1)\n",
    "for i in range(window, 0, -1):\n",
    "    cols.append(data.shift(i))\n",
    "    names += [('%s(t-%d)' % (col, i)) for col in data.columns]\n",
    "# Current timestep (t=0)\n",
    "cols.append(data)\n",
    "names += [('%s(t)' % (col)) for col in data.columns]\n",
    "# Target timestep (t=lag)\n",
    "cols.append(data.shift(-lag))\n",
    "names += [('%s(t+%d)' % (col, lag)) for col in data.columns]\n",
    "# Put it all together\n",
    "agg = pd.concat(cols, axis=1)\n",
    "agg.columns = names\n",
    "# Drop rows with NaN values\n",
    "if dropnan:\n",
    "    agg.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f409d77-efb2-4add-9aa6-314915ec1083",
   "metadata": {},
   "outputs": [],
   "source": [
    "item(t-%d)' % 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cac956-ae40-4b09-a37f-97544375660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_for_dates(date_list):\n",
    "    result_dict = dict()\n",
    "    for start_str in date_list:\n",
    "        start_date = datetime.datetime(*list(map(lambda x: int(x), start_str.split('-'))))\n",
    "        end_date = start_date + datetime.timedelta(days=days_len)\n",
    "        end_str = end_date.strftime('%Y-%m-%d')\n",
    "        for item in items:\n",
    "            abs_parts = []\n",
    "            dd = start_date\n",
    "            while dd < end_date:\n",
    "                abs_parts.append(list(filter(lambda x: f':item{item}-' in x, abs_data.loc[abs_data['date'] == dd.strftime('%Y-%m-%d'), 'seq_cnv'])))\n",
    "                dd += datetime.timedelta(days=1)\n",
    "            input_parts = train.loc[train['item'] == item].loc[train['date'] >= start_str].loc[train['date'] < end_str].sort_values(by=['date'])\n",
    "            input_pivot = input_parts.pivot(index='date', columns='store', values='sales')\n",
    "            output_parts = train.loc[train['item'] == item].loc[train['date'] == end_str].pivot(index='date', columns='store', values='sales')\n",
    "            if item not in result_dict.keys():\n",
    "                result_dict[item] = dict()\n",
    "            result_dict[item][start_str] = {\n",
    "                'store_seq': abs_parts,\n",
    "                'input': list(map(lambda x: list(map(lambda y: int(y), list(x))), input_pivot.values.T)),\n",
    "                'output': list(map(lambda x: int(x), output_parts.values.squeeze())),\n",
    "                'total': list(map(lambda x: int(x), input_pivot.sum(1).values))\n",
    "            }\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5c079e-5765-4a94-aa48-dcb6617e6e80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "n_jobs, n_len = 20, 5\n",
    "\n",
    "for idx, filename in zip([train_idx, val_idx, test_idx], ['train', 'val', 'test']):\n",
    "    n_block = round(len(idx) / n_len)\n",
    "    splits = np.array_split(idx, n_block)\n",
    "    \n",
    "    data_dict_list = []\n",
    "    for ii in range(0, len(splits), n_jobs):\n",
    "        tmp = splits[ii:ii + n_jobs]\n",
    "        print(filename, list(map(lambda x: len(x), tmp)))\n",
    "        start_time = datetime.datetime.now()\n",
    "        data_dict_list += Parallel(n_jobs=len(tmp))(delayed(process_for_dates)(dates) for dates in tmp)\n",
    "        print(ii, datetime.datetime.now() - start_time)\n",
    "    for i in range(len(data_dict_list)):\n",
    "        if i == 0:\n",
    "            merged_dict = data_dict_list[0]\n",
    "        else:\n",
    "            for k, v in data_dict_list[i].items():\n",
    "                for date in v.keys():\n",
    "                    merged_dict[k][date] = v[date]\n",
    "    with open(f'sales_data/{filename}_7days.json', 'w') as f:\n",
    "        json.dump(merged_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a5fe14-f44f-45e0-9618-d86feb896fca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for name in ['train', 'val', 'test']:\n",
    "    path = f'sales_data/{name}_items'\n",
    "    if not os.path.isdir(path):\n",
    "        os.system(f'mkdir {path}')\n",
    "    data = json.load(open(f'sales_data/{name}_7days.json', 'r'))\n",
    "    for item in items:\n",
    "        item_data = data[str(item)]\n",
    "        dates = item_data.keys()\n",
    "        remove_dates = []\n",
    "        for d in dates:\n",
    "            if [] in item_data[d]['store_seq']:\n",
    "                remove_dates.append(d)\n",
    "        print(remove_dates)\n",
    "        for d in remove_dates:\n",
    "            del item_data[d]\n",
    "        with open(f'{path}/item{item}.json', 'w') as f:\n",
    "            json.dump(item_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179081f1-0f4e-4a3c-89c9-f8c2a5eaf077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "items = sorted(train['item'].drop_duplicates())\n",
    "for item in items:\n",
    "    for name in ['train', 'val', 'test']:\n",
    "        data_dict = json.load(open(f'sales_data/{name}_items/item{item}.json', 'r'))\n",
    "        dates = data_dict.keys()\n",
    "        df_dict = dict()\n",
    "        for col in ['store_seq', 'input', 'output', 'total']:\n",
    "            data = list(map(lambda x: data_dict[x][col], dates))\n",
    "            df_dict[col] = data\n",
    "        df = pd.DataFrame(df_dict)\n",
    "        df['date'] = dates\n",
    "        df_json = df.to_dict('records')\n",
    "        with open(f'sales_data/{name}_items/Item{item}.json', 'w') as f:\n",
    "            json.dump(df_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8a1467-4e20-40e4-bcd8-13b33928704a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import ElectraModel, ElectraTokenizer\n",
    "\n",
    "MODEL_PATH = 'sales_seq_electra-small_model'\n",
    "TOK_PATH = 'koelectra_tokenizer'\n",
    "tokenizer = ElectraTokenizer.from_pretrained(TOK_PATH)\n",
    "bert = ElectraModel.from_pretrained(MODEL_PATH)\n",
    "\n",
    "\n",
    "def bert_model(seq_data):\n",
    "    bert_input = []\n",
    "    for seq_list in seq_data:\n",
    "        token_list = []\n",
    "        for seqs in seq_list:\n",
    "            token_list.append(tokenizer.batch_encode_plus(\n",
    "                seqs,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=32,\n",
    "                return_tensors='pt'\n",
    "            ).input_ids.numpy())\n",
    "        bert_input.append(np.stack(token_list))\n",
    "    return np.stack(bert_input)#.astype(np.float16)\n",
    "\n",
    "def scale(arr_list):\n",
    "    arr_list = np.array(arr_list)\n",
    "    sizes = list(arr_list.shape)\n",
    "\n",
    "    if len(sizes) == 2:\n",
    "        sizes += [1]\n",
    "        arr_list = np.vstack(arr_list.reshape(*sizes))\n",
    "    else:\n",
    "        arr_list = np.concatenate(np.vstack(arr_list)).reshape(-1, 1)\n",
    "    transed = pow_scaler.transform(arr_list).reshape(*sizes)\n",
    "    if sizes[-1] != 1:\n",
    "        sizes_ = sizes[:1] + [1] + sizes[1:]\n",
    "        transed = transed.reshape(*sizes_)\n",
    "    return np.array(transed, dtype=np.float16)\n",
    "\n",
    "def process_file(batch):\n",
    "    return {\n",
    "        #'input_': scale(batch['input']),\n",
    "        #'output_': scale(batch['output']),\n",
    "        #'total_': scale(batch['total']),\n",
    "        'bert_input': bert_model(batch['store_seq'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b4f957-e85b-41c8-80f3-0896d75a407e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "datasets = load_dataset('json', data_files={'test': 'sales_data/train_items/Item1.json'})\n",
    "testdatasets = datasets.map(\n",
    "    process_file,\n",
    "    batched=True,\n",
    "    num_proc=2,\n",
    "    load_from_cache_file=True,\n",
    "    remove_columns=['store_seq', 'date'], # 'input', 'output', 'total', \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f98259-2809-4d62-bb97-465899804364",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.randn(26, 10, 20)\n",
    "a.mean(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670cd398-4851-40d1-8ba7-816be9288585",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testdatasets['test'].data.to_pandas()['bert_input'].iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f9b0a1-42c1-4dfe-a65d-7a102c1623ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testdatasets['test'].data.to_pandas()['input_'].iloc[:2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1e6975-2ba7-4053-9a2f-3bb1acbfe306",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = np.random.rand(2, 3, 5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb32be4d-6bde-4289-a599-a15da7cbf551",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.array([aa.T for aa in a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b433d85d-82de-48b8-8d2e-23ac930612d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.FloatTensor(np.stack(testdatasets['test'].data.to_pandas()['output'].iloc[0])).half().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31c4301-4dc3-4535-807e-194077e92fc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "train.loc[train['date'] < '2016-01-01'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b246abd-4a63-4c67-9428-5e18fbfffa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train.loc[train['date'] < '2016-01-01', 'sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90b1c0e-db44-4803-b200-599f473aa306",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "\n",
    "std_scaler = StandardScaler().fit(train.loc[train['date'] < '2016-01-01', 'sales'].values.reshape(-1, 1))\n",
    "pow_scaler = PowerTransformer().fit(train.loc[train['date'] < '2016-01-01', 'sales'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93be9eb5-fdab-4fe4-884c-42c400698d10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "std_scaler.transform(train.loc[train['date'] < '2016-01-01', 'sales'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede81405-6551-4c56-8a68-912ec2510fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(std_scaler.transform(train.loc[train['date'] >= '2016-01-01', 'sales'].values.reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3bc597-56b7-448c-aa84-552bd80cbc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(pow_scaler.transform(train.loc[train['date'] < '2016-01-01', 'sales'].values.reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b235aca9-e4b9-4481-951a-3b7ec3a180c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(pow_scaler.transform(train.loc[train['date'] >= '2016-01-01', 'sales'].values.reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18359af1-9cba-452e-85c5-c50905e97e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#pickle.load(std_scaler, open('sales_data/standard_transformer.pkl', 'wb'))\n",
    "pow_scaler = pickle.load(open('sales_data/power_transformer.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07a3e3c-2f51-42e3-9f2d-1d2e3e88f38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "gru = nn.GRU(10, 20, 7, batch_first=True, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eadd0ec-024b-40db-8bd3-12153b7a047b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gru.to(torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75775c8e-92b1-4d17-b4bb-ea9ce4affdd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.stack([torch.tensor([-1.5941, -1.3562]), torch.tensor([-1.1443, -0.7769])]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01863a37-a0af-4c34-be33-8da6b4fc4b91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
