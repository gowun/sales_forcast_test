{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "train_ = pickle.load(open('sales_data/train_7days.pkl', 'rb'))\n",
    "val_ = pickle.load(open('sales_data/val_7days.pkl', 'rb'))\n",
    "test_ = pickle.load(open('sales_data/test_7days.pkl', 'rb'))\n",
    "\n",
    "train_idx, val_idx, test_idx = list(train_.keys()), list(val_.keys()), list(test_.keys())\n",
    "\n",
    "abs_data = pd.concat([\n",
    "    pd.read_csv('train_bert_data/train_7days.csv'),\n",
    "    pd.read_csv('train_bert_data/val_7days.csv'),\n",
    "    pd.read_csv('train_bert_data/test_7days.csv')\n",
    "])\n",
    "print(abs_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_date, max_date = datetime.datetime(2013, 1, 1), datetime.datetime(2017, 12, 31)\n",
    "days_len = 7\n",
    "items = sorted(train['item'].drop_duplicates())\n",
    "\n",
    "train_dict, val_dict, test_dict = dict(), dict(), dict()\n",
    "start_date = min_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.loc[train['item'] == item].loc[train['date'] >= start_str].loc[train['date'] < end_str].sort_values(by=['date']).pivot(index='date', columns='store', values='sales').sum(1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(filter(lambda x: f':item{item}-' in x, abs_data.loc[abs_data['date'] == dd.strftime('%Y-%m-%d'), 'seq_cnv']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "train_ = pickle.load(open('sales_data/train_7days.pkl', 'rb'))\n",
    "val_ = pickle.load(open('sales_data/val_7days.pkl', 'rb'))\n",
    "test_ = pickle.load(open('sales_data/test_7days.pkl', 'rb'))\n",
    "\n",
    "train_idx, val_idx, test_idx = list(train_.keys()), list(val_.keys()), list(test_.keys())\n",
    "\n",
    "abs_data = pd.concat([\n",
    "    pd.read_csv('train_bert_data/train_7days.csv'),\n",
    "    pd.read_csv('train_bert_data/val_7days.csv'),\n",
    "    pd.read_csv('train_bert_data/test_7days.csv')\n",
    "])\n",
    "print(abs_data.shape)\n",
    "\n",
    "min_date, max_date = datetime.datetime(2013, 1, 1), datetime.datetime(2017, 12, 31)\n",
    "days_len = 7\n",
    "items = sorted(train['item'].drop_duplicates())\n",
    "\n",
    "train_dict, val_dict, test_dict = dict(), dict(), dict()\n",
    "start_date = min_date\n",
    "while True:\n",
    "    end_date = start_date + datetime.timedelta(days=days_len)\n",
    "    if end_date > max_date:\n",
    "        break\n",
    "    else:\n",
    "        start_str, end_str = start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')\n",
    "        for item in items:\n",
    "            abs_parts = []\n",
    "            dd = start_date\n",
    "            while dd < end_date:\n",
    "                abs_parts.append(list(filter(lambda x: f':item{item}-' in x, abs_data.loc[abs_data['date'] == dd.strftime('%Y-%m-%d'), 'seq_cnv'])))\n",
    "                dd += datetime.timedelta(days=1)\n",
    "            input_parts = train.loc[train['item'] == item].loc[train['date'] >= start_str].loc[train['date'] < end_str].sort_values(by=['date'])\n",
    "            input_pivot = input_parts.pivot(index='date', columns='store', values='sales')\n",
    "            output_parts = train.loc[train['item'] == item].loc[train['date'] == end_str].pivot(index='date', columns='store', values='sales')\n",
    "            part = {\n",
    "                'store_seq': abs_parts,\n",
    "                'input': list(map(lambda x: list(map(lambda y: int(y), list(x))), input_pivot.values.T)),\n",
    "                'output': list(map(lambda x: int(x), output_parts.values.squeeze())),\n",
    "                'total': list(map(lambda x: int(x), input_pivot.sum(1).values))\n",
    "            }\n",
    "            if start_str in train_idx:\n",
    "                if item not in train_dict.keys():\n",
    "                    train_dict[item] = dict()\n",
    "                train_dict[item][start_str] = part\n",
    "            elif start_str in val_idx:\n",
    "                if item not in val_dict.keys():\n",
    "                    val_dict[item] = dict()\n",
    "                val_dict[item][start_str] = part\n",
    "            else:\n",
    "                if item not in test_dict.keys():\n",
    "                    test_dict[item] = dict()\n",
    "                test_dict[item][start_str] = part\n",
    "    print(start_str, len(train_dict), len(val_dict), len(test_dict))\n",
    "    start_date += datetime.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filenames = list(map(lambda x: f'sales_data/{x}_7days.json', ['train', 'val', 'test']))\n",
    "for fi in filenames:\n",
    "    with open(fi, 'w') as f:\n",
    "        if 'train' in fi:\n",
    "            json.dumps(train_dict, f)\n",
    "        elif 'val' in fi:\n",
    "            json.dumps(val_dict, f)\n",
    "        else:\n",
    "            json.dumps(test_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val = json.loads(open('sales_data/val_7days.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "train_dict, test_dict = dict(), dict()\n",
    "for k in item_sales_dict.keys():\n",
    "    if k < '2016-01-01':\n",
    "        train_dict[k] = item_sales_dict[k]\n",
    "    else:\n",
    "        test_dict[k] = item_sales_dict[k]\n",
    "len(item_sales_dict), len(train_dict), len(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dict = dict()\n",
    "val_dates = np.random.choice(list(train_dict.keys()), round(len(train_dict) * 0.1), replace=False)\n",
    "val_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in val_dates:\n",
    "    val_dict[d] = train_dict[d]\n",
    "    del train_dict[d]\n",
    "len(train_dict), len(val_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "train = pickle.load(open('sales_data/train_7days.pkl', 'rb'))\n",
    "val = pickle.load(open('sales_data/val_7days.pkl', 'rb'))\n",
    "test = pickle.load(open('sales_data/test_7days.pkl', 'rb'))\n",
    "val.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_data = pd.concat([\n",
    "    pd.read_csv('train_bert_data/train_7days.csv'),\n",
    "    pd.read_csv('train_bert_data/val_7days.csv'),\n",
    "    pd.read_csv('train_bert_data/test_7days.csv')\n",
    "])\n",
    "abs_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for store in train['store'].drop_duplicates():\n",
    "    print(f'train store: {store}, items: {\", \".join(train.loc[train[\"store\"] == store, \"item\"].drop_duplicates().astype(str))}')\n",
    "    print(f'test store: {store}, items: {\", \".join(test.loc[test[\"store\"] == store, \"item\"].drop_duplicates().astype(str))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "min_sales, max_sales = 0, 300\n",
    "range_ = 10\n",
    "sales_blocks = dict()\n",
    "base = ord('a')\n",
    "for j, i in enumerate(np.arange(min_sales, max_sales, range_)):\n",
    "    low, high = i, i + range_ - 1\n",
    "    sales_blocks[(low, high)] = chr(base + j)\n",
    "sales_blocks[(max_sales, 1000000)] = chr(base + len(sales_blocks))\n",
    "sales_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_code(sales_blocks, x):\n",
    "    for k, v in sales_blocks.items():\n",
    "        if x >= k[0] and x <= k[1]:\n",
    "            return sales_blocks[k]\n",
    "\n",
    "train['sales_processed'] = list(map(lambda x: convert_code(sales_blocks, x), train['sales']))\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('data/train_converted.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import datetime\n",
    "\n",
    "min_date, max_date = datetime.datetime(2013, 1, 1), datetime.datetime(2017, 12, 31)\n",
    "days_len = 7\n",
    "train['sales'] = train['sales'].astype(str)\n",
    "\n",
    "for store in train['store'].drop_duplicates():\n",
    "    start_time = datetime.datetime.now()\n",
    "    dates, inputs_converted, inputs_org = [], [], []\n",
    "    start_date = min_date\n",
    "    while True:\n",
    "        end_date = start_date + datetime.timedelta(days=days_len)\n",
    "        if end_date > max_date:\n",
    "            break\n",
    "\n",
    "        start_str, end_str = start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')\n",
    "        input_parts = train.loc[train['store'] == store].loc[train['date'] >= start_str].loc[train['date'] < end_str].sort_values(by=['date', 'item'])\n",
    "        output_parts = train.loc[train['store'] == store].loc[train['date'] == end_str]\n",
    "        input_ = f'{store}:{start_str[5:]}'\n",
    "        for item in input_parts['item'].drop_duplicates():\n",
    "            inputC = f'date{start_str[5:]}:store{store}:item{item}-' + ''.join(input_parts.loc[input_parts['item'] == item, 'sales_processed'].values)\n",
    "            inputO = f'date{start_str[5:]}:store{store}:item{item}-' + '>'.join(input_parts.loc[input_parts['item'] == item, 'sales'].values)\n",
    "            dates.append(start_str)\n",
    "            inputs_converted.append(inputC)\n",
    "            inputs_org.append(inputO)\n",
    "        start_date = start_date + datetime.timedelta(days=1)\n",
    "    records = pd.DataFrame({'date': dates, 'seq_org': inputs_org, 'seq_cnv': inputs_converted})\n",
    "    records.to_csv(f'data/store{store}_{days_len}days_converted.csv', index=False)\n",
    "    print(f'dur: {datetime.datetime.now() - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_store, std_date = 10, '2016-01-01'\n",
    "train, test = [], []\n",
    "for i in range(n_store):\n",
    "    whole = pd.read_csv(f'data/store{i + 1}_{days_len}days_converted.csv')\n",
    "    train.append(whole.loc[whole['date'] < std_date])\n",
    "    test.append(whole.loc[whole['date'] >= std_date])\n",
    "train = pd.concat(train).reset_index(drop=True)\n",
    "test = pd.concat(test).reset_index(drop=True)\n",
    "val_idx = np.random.choice(np.arange(len(train)), round(len(train) * 0.1), replace=False)\n",
    "train_idx = set(np.arange(len(train))) - set(val_idx)\n",
    "val = train.iloc[sorted(val_idx)]\n",
    "train = train.iloc[sorted(train_idx)]\n",
    "train.to_csv(f'train_bert_data/train_{days_len}days.csv', index=False)\n",
    "val.to_csv(f'train_bert_data/val_{days_len}days.csv', index=False)\n",
    "test.to_csv(f'train_bert_data/test_{days_len}days.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import BertForMaskedLM, AutoTokenizer, pipeline\n",
    "from skt.vault_utils import get_secrets\n",
    "\n",
    "proxy = get_secrets(\"proxy\")[\"proxy\"]\n",
    "os.environ['HTTP_PROXY'] = proxy\n",
    "os.environ['HTTPS_PROXY'] = proxy\n",
    "\n",
    "bert = BertForMaskedLM.from_pretrained('distilbert-base-uncased').cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=bert,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert.cpu()\n",
    "bert.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = tokenizer(\"Hello, my [MASK] is cute\", return_tensors=\"pt\")\n",
    "print(inputs.input_ids)\n",
    "with torch.no_grad(): \n",
    "    outputs = bert(**inputs)\n",
    "outputs.prediction_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = torch(\"The body was recovered and sent to Cox's Bazar [MASK] morgue for autopsy\")\n",
    "text.cuda()\n",
    "#fill_mask(\"The body was recovered and sent to Cox's Bazar [MASK] morgue for autopsy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fill_mask(\"Hello, my [MASK] is cute\")[0]['sequence'][6:-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_ids = tokenizer(list(val['seq_org'])).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from skt.vault_utils import get_secrets\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "proxy = get_secrets(\"proxy\")[\"proxy\"]\n",
    "os.environ['HTTP_PROXY'] = proxy\n",
    "os.environ['HTTPS_PROXY'] = proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, ElectraTokenizer, AutoConfig\n",
    "\n",
    "\n",
    "model = AutoModel.from_pretrained(\"sales_seq_electra-small_model\")\n",
    "#lmmodel = AutoModelForMaskedLM.from_pretrained(\"monologg/koelectra-base-v3-generator\")\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"koelectra_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\"sales_seq_electra-small_model\", return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "input_ids = tokenizer(\"나는 식당에서 밥을 먹었다.\").input_ids\n",
    "md_result = model(torch.tensor([input_ids]))\n",
    "#lmmd_result = lmmodel(torch.tensor([input_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "md_result.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_result.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lmmd_result = lmmodel.electra(torch.tensor([input_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lmmd_result.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"monologg/koelectra-base-v3-generator\",\n",
    "    tokenizer=\"monologg/koelectra-base-v3-generator\"\n",
    ")\n",
    "\n",
    "fill_mask(\"나는 {} 밥을 먹었다.\".format(fill_mask.tokenizer.mask_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model='roberta-base'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask(\"The man worked as a <mask>.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "embedding = nn.Embedding(7, 3)\n",
    "input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 5]])\n",
    "a = embedding(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "gru = nn.GRU(10, 20, 7, batch_first=True, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = torch.FloatTensor(np.stack([val['2014-01-09'][1]['input'].T, val['2014-01-09'][2]['input'].T]))\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.randn(7, 2, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c, d = gru(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_ids = tokenizer.batch_encode_plus(val['2014-01-09'][1]['store_seq'][:2], padding='max_length', max_length=50, truncation=True).input_ids\n",
    "model(torch.tensor(input_ids)).last_hidden_state[:, 0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aa = torch.stack(list(map(lambda x: model(torch.tensor(input_ids)).last_hidden_state[:, x, :].sum(0), range(7))))\n",
    "bb = torch.stack(list(map(lambda x: model(torch.tensor(input_ids)).last_hidden_state[:, x, :].sum(0), range(7))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cc = torch.stack([aa, bb])\n",
    "cc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cat([c, cc], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_layer = nn.Linear(config.hidden_size, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert_layer(cc)[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "load_dataset('pickle', 'sales_data/val_7days.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json.dumps(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json.dumps({'a': [[1, 2, 3], [4, 5, 6]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(val['2014-01-09'][1]['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
